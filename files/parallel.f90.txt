!!!====================================================================================
!!! 1. We use the following codes to allocate workload to different CPUs in HPCPOWER system
!!!    in HKU.
!!! 2. After finishing typing, we use commond 'fortimsl90 parallel' to compile it and 
!!!    a file 'parallel.exe' will be generated. 
!!! 3. After compiling and before submit it to HPCPOWER system formally, we can use
!!!    'mpirun -np 4 ./parallel.exe' to test it.
!!!====================================================================================
PROGRAM main
  IMPLICIT NONE
  INTEGER,PARAMETER::DOUBLE = KIND(0.0d0)
  INTEGER i,j,jj,iseed0,iseed
  REAL(DOUBLE) a(2,2),sum_a(2,2),a1(2,2)

!=============(standard mpi initializaiton)=======================================
!
  INCLUDE 'mpif.h'
  INTEGER rank,numtasks,rc,ierr,master

  CALL mpi_init(ierr)
  IF(ierr.NE.0) THEN
     PRINT*, 'Error starting MPI program. Terminating.'
     CALL mpi_abort(mpi_comm_world,rc,ierr)
  END IF
  CALL mpi_comm_rank(mpi_comm_world,rank,ierr)
  CALL mpi_comm_size(mpi_comm_world,numtasks,ierr)
  master=0
  PRINT*, '*total no. of task',numtasks,'rank=',rank
!!!============================================================================================
!!! 1. 'mpif.h' is a head file, which is needed for parallelling. 
!!! 2.  Suppose 10 CPUs are used in work, then there are 10 CPUs are occupied and ten numbers
!!!     0,1,...,9 are assigned to them. The variable 'rank' is just the number and numtasks=10. 
!!! 3.  Using 'master=0' to set the master CPU, which also take the job to collect the results
!!!     from other CPUs after the normal computation.

!!!=======================(main program)=======================================================
!
  iseed0 = 1000
  a1=0.0
  DO i = rank*4+1,(rank+1)*4
     iseed = iseed0+i
     DO j = 1,2
        DO jj = 1,2
           a(j,jj) = iseed
        END DO
     END DO
	 a1=a1+a
  END DO
!!!===========================================================================================
!!! The above codes will be run at each CPU and generate a 2 by 2 matrix at each CPU, i.e.
!!! there are totally 10 2 by 2 matrices generated and are all named 'a1'.

!!!===================(collecting result)=====================================================
!
  CALL mpi_reduce(a1,sum_a,2*2,mpi_double_precision,mpi_sum,master,mpi_comm_world,ierr)
  IF(rank==master) THEN
     OPEN(20,file = '/home/ligd/program/cr70/result.txt')
     DO i = 1,2
        DO j = 1,2
           WRITE(20,*) sum_a(i,j)
        END DO
     END DO
     CLOSE(20)
  ENDIF
!!!===========================================================================================
!!! We use the above code to collect the 10 matrices and take summation of them. We will 
!!! explain all these parameters listed in Subrountine 'mpi_reduce'.
!!! 1. 'a1': Tell 'mpi_reduce' that we want to operate 'a1' generated by each CPU;
!!! 2. 'sum_a': Use this variable to store collected results;
!!! 3. '2*2': 'sum_a' is a 2 by 2 matrix;
!!! 4. 'mpi_double_precision: 'sum_a' has double type;
!!! 5. 'mpi_sum': Take summation operation on these 'a1';
!!! 6. 'master': Let master CPU (or '0' CPU) to do collection)
!!!
!!! The predefined MPI reduction operations appear below. Users can also define their own reduction functions.
!!! -----------------------------------------------------------------
!!! MPI Reduction Operation   C data types       FORTRAN data types
!!! -----------------------------------------------------------------
!!! MPI_MAX    maximum       integer, float      integer, real, complex  
!!! MPI_MIN    minimum       integer, float      integer, real, complex  
!!! MPI_SUM    sum          integer, float      integer, real, complex  
!!! MPI_PROD    product      integer, float      integer, real, complex  
!!! MPI_LAND    logical and   integer           logical                 
!!! MPI_BAND    bit-wise and  integer, MPI_BYTE   integer, MPI_BYTE      
!!! MPI_LOR    logical or    integer            logical                 
!!! MPI_BOR    bit-wise or   integer, MPI_BYTE   integer, MPI_BYTE       
!!! MPI_LXOR    logical xor   integer           logical                 
!!! MPI_BXOR    bit-wise xor  integer, MPI_BYTE   integer, MPI_BYTE      
!!! MPI_MAXLOC  max value    combination of int  combination of integer     
!!!           and location  float, double and   real, complex,     
!!!                         long double         double precision        
!!! MPI_MINLOC  min value    combination of int  combination of integer     
!!!           and location  float, double and   real, complex,     
!!!                         long double         double precision        
!!! -----------------------------------------------------------------
!!!===========================================================================================
END PROGRAM main
